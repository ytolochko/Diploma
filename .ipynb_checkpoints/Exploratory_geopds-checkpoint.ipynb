{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import pandas as pd\n",
    "import re as re\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conflict = pd.read_csv('Data\\\\conflict_data\\\\DRC_2.csv', header = 0, index_col = 0)\n",
    "\n",
    "aid = pd.read_csv('Data\\\\aid_data\\\\data\\\\level_1a.csv')\n",
    "aid = aid.dropna(subset=['latitude', 'longitude']) # drop those entries that don't have coordinates to them\n",
    "\n",
    "worker_deaths = pd.read_csv('Data\\\\security_incidents.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "provinces = gpd.read_file('Data\\\\gadm36_COD_shp/gadm36_COD_1.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geometry = [Point(xy) for xy in zip(conflict.longitude, conflict.latitude)]\n",
    "gconflict = gpd.GeoDataFrame(conflict, crs = {'init': 'epsg:4326'}, geometry = geometry)\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(aid.longitude, aid.latitude)]\n",
    "gaid = gpd.GeoDataFrame(aid, crs = {'init': 'epsg:4326'}, geometry = geometry)\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(worker_deaths.Longitude, worker_deaths.Latitude)]\n",
    "g_w_d = gpd.GeoDataFrame(worker_deaths, crs = {'init': 'epsg:4326'}, geometry = geometry) # g_w_d = gworker_deaths\n",
    "\n",
    "\n",
    "gconflict = gpd.sjoin(gconflict, provinces, how=\"inner\")\n",
    "gaid = gpd.sjoin(gaid, provinces, how=\"inner\")\n",
    "g_w_d = gpd.sjoin(g_w_d, provinces, how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the number of rows there column_name has more than one entry, separated by \"|\". We use this for donors and aid focus. \n",
    "def calc_multiples(df, column_name):\n",
    "    s = 0\n",
    "    l = []\n",
    "    for index, row in df.iterrows():\n",
    "        entries = row[column_name].split(\"|\")\n",
    "        if len(entries) > 1:\n",
    "            s += 1\n",
    "            l.append(index)\n",
    "    return s#, l\n",
    "# split those rows that have more than one entry in a certain column name.\n",
    "# We create a new row for each of the multiple entries (other entries being the same)\n",
    "# and delete the original multiple entry row. \n",
    "def split_rows(data, column_name):\n",
    "    df = copy.deepcopy(data)\n",
    "    temp = pd.DataFrame()\n",
    "    for index, row in df.iterrows():\n",
    "        entries = row[column_name].split(\"|\")\n",
    "        if len(entries) > 1:\n",
    "            for entry in entries:\n",
    "                temp_row = row\n",
    "                temp_row[column_name] = entry\n",
    "                temp = temp.append(temp_row)\n",
    "            df.drop(index, inplace = True)\n",
    "            \n",
    "    return df.append(temp)\n",
    "\n",
    "# split the rows(projects) that have more than one donor per project\n",
    "gaid = split_rows(gaid, 'donors')\n",
    "gaid = split_rows(gaid, 'ad_sector_names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('measures_indices'):\n",
    "    os.makedirs('measures_indices')\n",
    "    \n",
    "# how many projects there are per donor and their share in the total, save in a csv file  \n",
    "total = gaid['donors'].value_counts() # total N of projects per donor\n",
    "share = gaid['donors'].value_counts() / gaid.shape[0] # calculate share of the projects per donor in total N of projects\n",
    "pd.concat([total, share], axis = 1).to_csv('measures_indices\\\\N_projects_per_donor.csv')\n",
    "\n",
    "# what share of projects per donor have multiple focus sectors (e.g. General environmental protection|Transport and storage)\n",
    "multiple_focus_share = (gaid.groupby('donors').apply(calc_multiples, 'ad_sector_names') / gaid['donors'].value_counts())\n",
    "multiple_focus_share.to_csv('measures_indices\\\\project_multiple_sectors.csv')\n",
    "\n",
    "# share of each project focus in the total number of projects per donor. We use this to calculate variablity of project focus for a donor\n",
    "focus_share = (gaid.groupby(['donors', 'ad_sector_names']).size() / gaid.groupby(['donors']).size())\n",
    "focus_share.to_csv('measures_indices\\\\donor_sector_share.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Func calculates the coefficient of unalikeability (as defined by Kader 2007) of every sublcass of first_group variable by topic_name\n",
    "# E.g. unalikeability of project location for each aid donor -> calc_unalikeability(gaid, 'donors', 'ad_sector_names')\n",
    "\n",
    "def calc_unalikeability(data, first_group, topic_name):\n",
    "    \n",
    "    # prepare data: group by first_group and topic_name, and divide by the size of the respective group\n",
    "    # thus we obtain the share that each topic_name has in the respective first_group\n",
    "    \n",
    "    d = (data.groupby([first_group, topic_name]).size() / gaid.groupby([first_group]).size())\n",
    "    \n",
    "    # here we get the keys for the first level grouping. So, the unique values of first group column\n",
    "    keys = []\n",
    "    for i in d.index:\n",
    "        keys.append(i[0])\n",
    "    keys = set(keys)\n",
    "    \n",
    "    # here we calculate the actual coefficient\n",
    "    # for every value of first_group we calculate its coefficient:\n",
    "    # coefficient is defined as 1 - SUM_i(p_i^2), where p_i is the share of the ith subgroup in the total group. \n",
    "    \n",
    "    coefs = {}\n",
    "    for key in keys:\n",
    "        s = 0\n",
    "        for subgroup in d[key]:\n",
    "            s += subgroup ** 2\n",
    "        coef = 1 - s\n",
    "        coefs[key] = coef\n",
    "    return pd.Series(coefs)\n",
    "\n",
    "\n",
    "# calculates the variablity of topic_name (e.g. total commitments of money) for every member of first_group\n",
    "def calc_var(data, first_group, topic_name):\n",
    "    d = data.groupby(first_group)\n",
    "    \n",
    "    def var(d):\n",
    "        d = d[topic_name]\n",
    "        d = (d - min(d)) / (max(d) - min(d))\n",
    "        \n",
    "        if np.isnan(np.var(d)):\n",
    "            return 0\n",
    "        \n",
    "        return np.var(d)\n",
    "    \n",
    "    return d.apply(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adaptability_by_focus = calc_unalikeability(gaid, 'donors', 'ad_sector_names')\n",
    "adaptability_by_location =calc_unalikeability(gaid, 'donors', 'NAME_1')\n",
    "adaptability_by_start_year = calc_unalikeability(gaid, 'donors', 'transactions_start_year')\n",
    "adaptability_by_commitment = calc_var(gaid, 'donors', 'total_commitments')\n",
    "composite_adaptability = adaptability_by_commitment + adaptability_by_start_year + adaptability_by_location + adaptability_by_focus\n",
    "#pd.DataFrame.from_dict(adaptability_by_focus, orient = 'index').to_csv('measures_indices\\\\adaptability_by_focus.csv')\n",
    "#pd.DataFrame.from_dict(adaptability_by_location, orient = 'index').to_csv('measures_indices\\\\adaptability_by_location.csv')\n",
    "#pd.DataFrame.from_dict(adaptability_by_start_year, orient = 'index').to_csv('measures_indices\\\\adaptability_by_start_year.csv')\n",
    "#adaptability_by_commitment.to_csv('measures_indices\\\\adaptability_by_commitments.csv')\n",
    "#(composite_adaptability).to_csv('measures_indices\\\\composite_adaptability.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide donors into 2 groups: higly adaptable and not higly adaptable, the division line is the median. \n",
    "median = composite_adaptability.median()\n",
    "high_adaptability = composite_adaptability[composite_adaptability >= median]\n",
    "low_adaptability = composite_adaptability[composite_adaptability < median]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the year, ADM1 pairs of events when aid workers were killed. \n",
    "# Then, we can assume that these are unreceptive year, region pairs.\n",
    "# However, we can also divide this into 2 subgroups: more than average (or median) casualties vs less than average (or median) casualties\n",
    "worker_deaths_grouped = g_w_d.groupby(['Year', 'NAME_1'])['Total affected'].sum()\n",
    "worker_deaths_grouped.to_csv('measures_indices\\\\year_location_pairs.csv')\n",
    "\n",
    "# The idea is that low_receptivity is depcited by a lot of worker deaths\n",
    "# high receptivity is depicted by a lesser number of worker deaths\n",
    "low_receptivity = g_w_d.groupby(['Year', 'NAME_1'])['Total affected'].sum() > worker_deaths_grouped.describe()['mean']\n",
    "high_receptivity = g_w_d.groupby(['Year', 'NAME_1'])['Total affected'].sum() < worker_deaths_grouped.describe()['mean']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(1998, 'Sankuru'), (2000, 'Ituri'), (2000, 'Sankuru'),\n",
       "       (2000, 'Sud-Kivu'), (2000, 'Tshopo'), (2001, 'Kongo-Central'),\n",
       "       (2001, 'Sud-Kivu'), (2002, 'Maniema'), (2002, 'Sankuru'),\n",
       "       (2003, 'Sankuru'), (2003, 'Tshopo'), (2004, 'Ituri'),\n",
       "       (2004, 'Maniema'), (2004, 'Nord-Kivu'), (2004, 'Sankuru'),\n",
       "       (2005, 'Ituri'), (2006, 'Ituri'), (2006, 'Nord-Kivu'),\n",
       "       (2006, 'Sankuru'), (2008, 'Ituri'), (2008, 'Sud-Kivu'),\n",
       "       (2009, 'Haut-Katanga'), (2009, 'Sud-Kivu'), (2010, 'Kinshasa'),\n",
       "       (2012, 'Ituri'), (2013, 'Ituri'), (2014, 'Bas-Uélé'),\n",
       "       (2014, 'Ituri'), (2015, 'Kasaï'), (2015, 'Kinshasa'),\n",
       "       (2016, 'Haut-Uélé'), (2016, 'Maniema'), (2016, 'Sud-Ubangi'),\n",
       "       (2016, 'Équateur'), (2017, 'Nord-Kivu'), (2018, 'Sud-Kivu')], dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_deaths_grouped[high_receptivity].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO LIST\n",
    "\n",
    "# 3. Select projects based on the 4 subcategories derived above.\n",
    "# Projects with: donor is in one of the first classification group.\n",
    "# Start year and area is in one of the second classification groups.\n",
    "\n",
    "#                        High adaptability              Low adaptability\n",
    "#\n",
    "# High receptivity           n                                 n2\n",
    "#\n",
    "# Low receptivity            n3                                n4\n",
    "\n",
    "\n",
    "\n",
    "# 4. THINK ABOUT INFERENCE PROBLEM, MATCHING PROBLEM, CREATING CONTROL AND TREATMENT GROUPS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
